{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sampling from the Variational Autoencoder\n",
    "In the exercise on autoencoders you plotted the hidden units of the autoencoer using TSNE. The autoencoders from the exercise (vanilla autoencoder, denosing autoencoder) are deterministic. On the other hand the variational autoencoder, implemented yesterday, learns a generative model where the latent variables typically have a standard normal prior. \n",
    "\n",
    "Because the variational autoencoder is a generative model we can sample from it by \n",
    "\n",
    " 1. z = p(z)  [sample each hidden unit from N(0,1 )]\n",
    " 2. $x_{sample} = decoder(z)$\n",
    " \n",
    "\n",
    "Where decoder is the decoding part of the variational autoencoder.\n",
    "\n",
    "In this notebook we learn a variational auto encoder with 2 latent variables. We'll create a plot where the x-axis is different values for $z_1$ and the y-axis is different values for $z_2$.\n",
    "\n",
    "For each $\\mathbf{z} = [z_1, z_2]$ we can use the above formule to draw a sample from the model. This allows us to see how samples drawn different regions of the $\\mathbf{z}$ space look. Sometimes you'll people will refer to this as the manifold. \n",
    "\n",
    "**Scroll to the bottom of the page to see samples drawn from 2d manifold that the VAE has learned**\n",
    "\n",
    "(With more turning you can create a figure similar to figure 4 in http://arxiv.org/pdf/1312.6114v10.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 1: Tesla K40c\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "#os.environ['THEANO_FLAGS'] = 'device=gpu1, floatX=float32'\n",
    "import theano\n",
    "theano.config.floatX = 'float32'\n",
    "import theano.tensor as T\n",
    "from theano.sandbox.rng_mrg import MRG_RandomStreams as RandomStreams\n",
    "from lasagne.nonlinearities import rectify\n",
    "from lasagne.layers.base import Layer\n",
    "from lasagne import init\n",
    "from lasagne.updates import adam\n",
    "from lasagne.layers import InputLayer, get_all_params\n",
    "import numpy as np\n",
    "import lasagne\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "c = - 0.5 * math.log(2*math.pi)\n",
    "def normal(x, mean, sd):\n",
    "    return c - T.log(T.abs_(sd)) - (x - mean)**2 / (2 * sd**2)\n",
    "\n",
    "def normal2(x, mean, logvar):\n",
    "    return c - logvar/2 - (x - mean)**2 / (2 * T.exp(logvar))\n",
    "\n",
    "def standard_normal(x):\n",
    "    return c - x**2 / 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct the lasagne layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class VAELayer(Layer):\n",
    "\n",
    "    def __init__(self, incoming, encoder, decoder,\n",
    "                 x_distribution='bernoulli',\n",
    "                 pz_distribution='gaussian',\n",
    "                 qz_distribution='gaussian',\n",
    "                 latent_size=50,\n",
    "                 W=init.Normal(0.01),\n",
    "                 b=init.Normal(0.01),\n",
    "                 **kwargs):\n",
    "        super(VAELayer, self).__init__(incoming, **kwargs)\n",
    "        num_batch, n_features = self.input_shape\n",
    "        self.num_batch = num_batch\n",
    "        self.n_features = n_features\n",
    "        self.x_distribution = x_distribution\n",
    "        self.pz_distribution = pz_distribution\n",
    "        self.qz_distribution = qz_distribution\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self._srng = RandomStreams()\n",
    "\n",
    "        if self.x_distribution not in ['gaussian', 'bernoulli']:\n",
    "            raise NotImplementedError\n",
    "        if self.pz_distribution not in ['gaussian', 'gaussianmarg']:\n",
    "            raise NotImplementedError\n",
    "        if self.qz_distribution not in ['gaussian', 'gaussianmarg']:\n",
    "            raise NotImplementedError\n",
    "\n",
    "        self.params_encoder = lasagne.layers.get_all_params(encoder)\n",
    "        self.params_decoder = lasagne.layers.get_all_params(decoder)\n",
    "        for p in self.params_encoder:\n",
    "            p.name = \"VAELayer encoder :\" + p.name\n",
    "        for p in self.params_decoder:\n",
    "            p.name = \"VAELayer decoder :\" + p.name\n",
    "\n",
    "        self.num_hid_enc = encoder.output_shape[1]\n",
    "        self.num_hid_dec = decoder.output_shape[1]\n",
    "        self.latent_size = latent_size\n",
    "\n",
    "        self.W_enc_to_z_mu = self.add_param(W, (self.num_hid_enc, latent_size))\n",
    "        self.b_enc_to_z_mu = self.add_param(b, (latent_size,))\n",
    "\n",
    "        self.W_enc_to_z_logsigma = self.add_param(W, (self.num_hid_enc, self.latent_size))\n",
    "        self.b_enc_to_z_logsigma = self.add_param(b, (latent_size,))\n",
    "\n",
    "        self.W_dec_to_x_mu = self.add_param(W, (self.num_hid_dec, self.n_features))\n",
    "        self.b_dec_to_x_mu = self.add_param(b, (self.n_features,))\n",
    "\n",
    "        self.W_params = [self.W_enc_to_z_mu,\n",
    "                         self.W_enc_to_z_logsigma,\n",
    "                         self.W_dec_to_x_mu] + self.params_encoder + self.params_decoder\n",
    "        self.bias_params = [self.b_enc_to_z_mu,\n",
    "                            self.b_enc_to_z_logsigma,\n",
    "                            self.b_dec_to_x_mu]\n",
    "\n",
    "        params_tmp = []\n",
    "        if self.x_distribution == 'gaussian':\n",
    "            self.W_dec_to_x_logsigma = self.add_param(W, (self.num_hid_dec, self.n_features))\n",
    "            self.b_dec_to_x_logsigma = self.add_param(b, (self.n_features,))\n",
    "            self.W_params += [self.W_dec_to_x_logsigma]\n",
    "            self.bias_params += [self.b_dec_to_x_logsigma]\n",
    "            self.W_dec_to_x_logsigma.name = \"VAE: W_dec_to_x_logsigma\"\n",
    "            self.b_dec_to_x_logsigma.name = \"VAE: b_dec_to_x_logsigma\"\n",
    "            params_tmp = [self.W_dec_to_x_logsigma, self.b_dec_to_x_logsigma]\n",
    "\n",
    "        self.params = self.params_encoder + [self.W_enc_to_z_mu,\n",
    "                                             self.b_enc_to_z_mu,\n",
    "                                             self.W_enc_to_z_logsigma,\n",
    "                                             self.b_enc_to_z_logsigma] + self.params_decoder + \\\n",
    "                      [self.W_dec_to_x_mu, self.b_dec_to_x_mu] + params_tmp\n",
    "\n",
    "        self.W_enc_to_z_mu.name = \"VAELayer: W_enc_to_z_mu\"\n",
    "        self.W_enc_to_z_logsigma.name = \"VAELayer: W_enc_to_z_logsigma\"\n",
    "        self.W_dec_to_x_mu.name = \"VAELayer: W_dec_to_x_mu\"\n",
    "        self.b_enc_to_z_mu.name = \"VAELayer: b_enc_to_z_mu\"\n",
    "        self.b_enc_to_z_logsigma.name = \"VAELayer: b_enc_to_z_logsigma\"\n",
    "        self.b_dec_to_x_mu.name = \"VAELayer: b_dec_to_x_mu\"\n",
    "\n",
    "    def get_params(self):\n",
    "        return self.params\n",
    "\n",
    "    def get_output_shape_for(self, input_shape):\n",
    "        dec_out_shp = self.decoder.get_output_shape_for(\n",
    "            (self.num_batch, self.num_hid_dec))\n",
    "        if self.x_distribution == 'bernoulli':\n",
    "            return dec_out_shp\n",
    "        elif self.x_distribution == 'gaussian':\n",
    "            return [dec_out_shp, dec_out_shp]\n",
    "\n",
    "    def _encoder_output(self, x, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        Get the output of the encoder of the layer.\n",
    "        :param x: the x input to the layer.\n",
    "        :return: the output of the encoder z.\n",
    "        \"\"\"\n",
    "        return lasagne.layers.get_output(self.encoder, x, **kwargs)\n",
    "\n",
    "    def decoder_output(self, z, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        Get the output of the decoder of the layer.\n",
    "        :param z: the latent z.\n",
    "        :return: the output of the decoder x reconstructed.\n",
    "        \"\"\"\n",
    "        h_decoder = lasagne.layers.get_output(self.decoder, z, **kwargs)\n",
    "        if self.x_distribution == 'gaussian':\n",
    "            mu_decoder = T.dot(h_decoder, self.W_dec_to_x_mu) + self.b_dec_to_x_mu\n",
    "            log_sigma_decoder = T.dot(h_decoder, self.W_dec_to_x_logsigma) + self.b_dec_to_x_logsigma\n",
    "            decoder_out = mu_decoder, log_sigma_decoder\n",
    "        elif self.x_distribution == 'bernoulli':\n",
    "            # TODO: Finish writing the output of the decoder for a bernoulli distributed x.\n",
    "            decoder_out = T.nnet.sigmoid(T.dot(h_decoder, self.W_dec_to_x_mu) + self.b_dec_to_x_mu)\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "        return decoder_out\n",
    "\n",
    "    def get_z_mu_sigma(self, x, *args, **kwargs):\n",
    "        h_encoder = self._encoder_output(x, *args, **kwargs)\n",
    "        mu_encoder = T.dot(h_encoder, self.W_enc_to_z_mu) + self.b_enc_to_z_mu\n",
    "        log_sigma_encoder = (T.dot(h_encoder, self.W_enc_to_z_logsigma) +\n",
    "                             self.b_enc_to_z_logsigma)\n",
    "        eps = self._srng.normal(log_sigma_encoder.shape)\n",
    "        # TODO: Calculate the sampled z. \n",
    "        z = mu_encoder + T.exp(0.5 * log_sigma_encoder) * eps\n",
    "        return z, mu_encoder, log_sigma_encoder\n",
    "\n",
    "    def get_log_distributions(self, x, *args, **kwargs):\n",
    "        # sample z from q(z|x).\n",
    "        h_encoder = self._encoder_output(x, *args, **kwargs)\n",
    "        mu_encoder = T.dot(h_encoder, self.W_enc_to_z_mu) + self.b_enc_to_z_mu\n",
    "        log_sigma_encoder = (T.dot(h_encoder, self.W_enc_to_z_logsigma) +\n",
    "                             self.b_enc_to_z_logsigma)\n",
    "        eps = self._srng.normal(log_sigma_encoder.shape)\n",
    "        z = mu_encoder + T.exp(0.5 * log_sigma_encoder) * eps\n",
    "\n",
    "        # forward pass z through decoder to generate p(x|z).\n",
    "        decoder_out = self.decoder_output(z, *args, **kwargs)\n",
    "        if self.x_distribution == 'bernoulli':\n",
    "            x_mu = decoder_out\n",
    "            log_px_given_z = -T.nnet.binary_crossentropy(x_mu, x)\n",
    "        elif self.x_distribution == 'gaussian':\n",
    "            x_mu, x_logsigma = decoder_out\n",
    "            log_px_given_z = normal2(x, x_mu, x_logsigma)\n",
    "\n",
    "        # sample prior distribution p(z).\n",
    "        if self.pz_distribution == 'gaussian':\n",
    "            log_pz = standard_normal(z)\n",
    "        elif self.pz_distribution == 'gaussianmarg':\n",
    "            log_pz = -0.5 * (T.log(2 * np.pi) + (T.sqr(mu_encoder) + T.exp(log_sigma_encoder)))\n",
    "\n",
    "        # variational approximation distribution q(z|x)\n",
    "        if self.qz_distribution == 'gaussian':\n",
    "            log_qz_given_x = normal2(z, mu_encoder, log_sigma_encoder)\n",
    "        elif self.qz_distribution == 'gaussianmarg':\n",
    "            log_qz_given_x = - 0.5 * (T.log(2 * np.pi) + 1 + log_sigma_encoder)\n",
    "\n",
    "        # sum over dim 1 to get shape (,batch_size)\n",
    "        log_px_given_z = log_px_given_z.sum(axis=1, dtype=theano.config.floatX)  # sum over x\n",
    "        log_pz = log_pz.sum(axis=1, dtype=theano.config.floatX)  # sum over latent vars\n",
    "        log_qz_given_x = log_qz_given_x.sum(axis=1, dtype=theano.config.floatX)  # sum over latent vars\n",
    "\n",
    "        return log_pz, log_qz_given_x, log_px_given_z\n",
    "\n",
    "    def draw_sample(self, z=None, *args, **kwargs):\n",
    "        if z is None:  # draw random z\n",
    "            z = self._srng.normal((self.num_batch, self.latent_size))\n",
    "        return self.decoder_output(z, *args, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct the model class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class VAE:\n",
    "    def __init__(self, n_in, n_hidden, n_out, n_hidden_decoder=None, trans_func=rectify, batch_size=100):\n",
    "        self.n_in = n_in\n",
    "        self.n_hidden = n_hidden\n",
    "        self.n_out = n_out\n",
    "        self.batch_size = batch_size\n",
    "        self.transf = trans_func\n",
    "        self.l_in = InputLayer(shape=(batch_size,n_in))\n",
    "\n",
    "        self.srng = RandomStreams()\n",
    "\n",
    "        l_in_encoder = lasagne.layers.InputLayer(shape=(batch_size, n_in))\n",
    "        l_in_decoder = lasagne.layers.InputLayer(shape=(batch_size, n_out))\n",
    "        l_prev_encoder = l_in_encoder\n",
    "        l_prev_decoder = l_in_decoder\n",
    "        for i in range(len(n_hidden)):\n",
    "            l_tmp_encoder = lasagne.layers.DenseLayer(l_prev_encoder,\n",
    "                                                      num_units=n_hidden[i],\n",
    "                                                      W=lasagne.init.Uniform(),\n",
    "                                                      nonlinearity=self.transf)\n",
    "        if n_hidden_decoder is None:\n",
    "            n_hidden_decoder = n_hidden\n",
    "        self.n_hidden_decoder = n_hidden_decoder\n",
    "        for i in range(len(n_hidden_decoder)):\n",
    "            l_tmp_decoder = lasagne.layers.DenseLayer(l_prev_decoder,\n",
    "                                                      num_units=n_hidden_decoder[-(i + 1)],\n",
    "                                                      W=lasagne.init.Uniform(),\n",
    "                                                      nonlinearity=self.transf)\n",
    "            l_prev_encoder = l_tmp_encoder\n",
    "            l_prev_decoder = l_tmp_decoder\n",
    "\n",
    "        l_in = lasagne.layers.InputLayer(shape=(batch_size, n_in))\n",
    "        self.model = VAELayer(l_in,\n",
    "                              encoder=l_prev_encoder,\n",
    "                              decoder=l_prev_decoder,\n",
    "                              latent_size=n_out,\n",
    "                              x_distribution='bernoulli',\n",
    "                              qz_distribution='gaussianmarg', #gaussianmarg\n",
    "                              pz_distribution='gaussianmarg')\n",
    "        self.x = T.matrix('x')\n",
    "\n",
    "    def build_model(self, train_x, test_x, valid_x, update, update_args):\n",
    "        self.train_x = train_x\n",
    "        self.test_x = test_x\n",
    "        self.validation_x = valid_x\n",
    "        self.update = update\n",
    "        self.update_args = update_args\n",
    "        self.index = T.iscalar('index')\n",
    "        self.batch_slice = slice(self.index * self.batch_size, (self.index + 1) * self.batch_size)\n",
    "\n",
    "        x = self.srng.binomial(size=self.x.shape, n=1, p=self.x)\n",
    "        log_pz, log_qz_given_x, log_px_given_z = self.model.get_log_distributions(self.x)\n",
    "        loss_eval = (log_pz + log_px_given_z - log_qz_given_x).sum()\n",
    "        loss_eval /= self.batch_size\n",
    "\n",
    "        all_params = get_all_params(self.model)\n",
    "        updates = self.update(-loss_eval, all_params, *self.update_args)\n",
    "\n",
    "        train_model = theano.function([self.index], loss_eval, updates=updates,\n",
    "                                      givens={self.x: self.train_x[self.batch_slice], },)\n",
    "\n",
    "        test_model = theano.function([self.index], loss_eval,\n",
    "                                     givens={self.x: self.test_x[self.batch_slice], },)\n",
    "\n",
    "        validate_model = theano.function([self.index], loss_eval,\n",
    "                                         givens={self.x: self.validation_x[self.batch_slice], },)\n",
    "\n",
    "        return train_model, test_model, validate_model\n",
    "\n",
    "    def draw_sample(self, z):\n",
    "        return self.model.draw_sample(z)\n",
    "\n",
    "    def get_output(self, dat):\n",
    "        z, _, _ = self.model.get_z_mu_sigma(dat)\n",
    "        return z\n",
    "\n",
    "    def get_reconstruction(self, z):\n",
    "        return self.model.decoder_output(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the MNIST data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from random import shuffle\n",
    "def load_mnist():\n",
    "        \n",
    "    data = np.load('mnist.npz')\n",
    "    num_classes = 10\n",
    "    x_train, targets_train = data['X_train'].astype('float32'), data['y_train']\n",
    "    x_valid, targets_valid = data['X_valid'].astype('float32'), data['y_valid']\n",
    "    x_test, targets_test = data['X_test'].astype('float32'), data['y_test']\n",
    "\n",
    "\n",
    "    def shared_dataset(x, y, borrow=True):\n",
    "        shared_x = theano.shared(np.asarray(x, dtype=theano.config.floatX), borrow=borrow)\n",
    "        shared_y = theano.shared(np.asarray(y, dtype=theano.config.floatX), borrow=borrow)\n",
    "        return shared_x, shared_y\n",
    "\n",
    "    return shared_dataset(x_train, targets_train), shared_dataset(x_test, targets_test), shared_dataset(x_valid, targets_valid)\n",
    "\n",
    "\n",
    "(train_x, train_t), (test_x, test_t), (valid_x, valid_t) = load_mnist()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "model = VAE(784, [200, 200], 2, trans_func=rectify, batch_size=100)\n",
    "\n",
    "sh_lr = theano.shared(lasagne.utils.floatX(1e-3))\n",
    "train_model, test_model, valid_model = model.build_model(train_x, test_x, valid_x, adam, update_args=(sh_lr,))\n",
    "\n",
    "\n",
    "import time\n",
    "batch_size = 500\n",
    "n_epochs = 200\n",
    "eval_train = {}\n",
    "eval_test = {}\n",
    "eval_valid = {}\n",
    "n_train_batches = train_x.get_value(borrow=True).shape[0] / batch_size\n",
    "n_test_batches = test_x.get_value(borrow=True).shape[0] / batch_size\n",
    "n_valid_batches = valid_x.get_value(borrow=True).shape[0] / batch_size\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    start_time = time.time()\n",
    "    avg_costs = []\n",
    "    for minibatch_index in xrange(n_train_batches):\n",
    "        minibatch_avg_cost = train_model(minibatch_index)\n",
    "        avg_costs.append(minibatch_avg_cost)\n",
    "    eval_train[epoch] = np.mean(avg_costs)\n",
    "    test_losses = [test_model(i) for i in xrange(n_test_batches)]\n",
    "    valid_losses = [valid_model(i) for i in xrange(n_valid_batches)]\n",
    "    eval_test[epoch] = np.mean(test_losses)\n",
    "    eval_valid[epoch] = np.mean(valid_losses)\n",
    "    end_time = time.time() - start_time\n",
    "    print \"[epoch,time,train,valid,test];%i;%.2f;%.10f;%.10f;%.10f\" % (epoch, end_time, eval_train[epoch], eval_valid[epoch], eval_test[epoch])\n",
    "    log_pz, log_qz_given_x, log_px_given_z = model.model.get_log_distributions(test_x)\n",
    "    \n",
    "    if (epoch+1) % 50 == 0:\n",
    "        sh_lr.set_value(lasagne.utils.floatX(sh_lr.get_value()*0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt # Plotting library.\n",
    "import matplotlib\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Exercise 5\n",
    "Plot samples from the $z$ distribution using a mesh like Kingma in http://arxiv.org/pdf/1312.6114v10.pdf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-b050d749ebed>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstats\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnorm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstats\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0minvgauss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinspace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.9\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnorm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mppf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "from scipy.stats import norm\n",
    "from scipy.stats import invgauss\n",
    "x = np.linspace(0.1,0.9, 20)\n",
    "v = norm.ppf(x)\n",
    "z = np.zeros((20**2, 2))\n",
    "i = 0\n",
    "for a in v:\n",
    "    for b in v:\n",
    "        z[i,0] = a\n",
    "        z[i,1] = b\n",
    "        i += 1\n",
    "z = z.astype('float32')     \n",
    "\n",
    "\n",
    "samples = model.draw_sample(z).eval()\n",
    "\n",
    "idx = 0\n",
    "canvas = np.zeros((28*20, 20*28))\n",
    "for i in range(20):\n",
    "    for j in range(20):\n",
    "        canvas[i*28:(i+1)*28, j*28:(j+1)*28] = samples[idx].reshape((28, 28))\n",
    "        idx += 1\n",
    "plt.figure(figsize=(7, 7))\n",
    "plt.imshow(1-canvas, cmap='gray')\n",
    "plt.title('MNIST handwritten digits')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
